{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "3eaf228a-e9e5-47cf-a96e-af7e87f8bd55",
    "_uuid": "190059f3-e243-4669-b104-bc6ec8493fae",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Capstone: RAG Pipeline Optimization and Evaluator Reliability\n",
    "\n",
    "Author: Darpan Beri, darpanberi (dot) 99 (at) gmail (dot) com\n",
    "\n",
    "Last Updated: 2025-04-22"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook implements the experiment pipeline for my capstone project.\n",
    "It involves:\n",
    "1. Setting up the environment and loading necessary libraries and data.\n",
    "2. Preprocessing text data and creating a knowledge base using Haystack.\n",
    "3. Defining a RAG pipeline using a Llama 3.1 8B model for answer generation.\n",
    "4. Defining an evaluation pipeline using another Llama 3.1 8B model to assess generated answers.\n",
    "5. Running experiments by varying `chunk_size` and `top_k` hyperparameters.\n",
    "6. Saving the results for analysis.\n",
    "\n",
    "**Note:** You need a HuggingFace token and access to the Llama 3.1 8B gated model granted to you by Meta. Request the model access from [here](https://huggingface.co/meta-llama/Llama-3.1-8B)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "0fdb6408-9797-4fd0-999f-f327e769379d",
    "_uuid": "c1a893cf-813c-4bbc-86b4-4e5a015ff5d9",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Dependencies\n",
    "This section installs the required Python packages for the project.\n",
    "- `datasets`: For loading data from Hugging Face Hub.\n",
    "- `ftfy`: For fixing text encoding issues.\n",
    "- `haystack-ai`: The core library for building the RAG pipeline (document stores, retrievers, embedders).\n",
    "- `sentence-transformers`: Used for embedding documents and queries.\n",
    "- `bitsandbytes`: Enables model quantization (loading models in 8-bit) to save memory.\n",
    "- `transformers`, `accelerate`: Hugging Face libraries for loading and running LLMs.\n",
    "- `tqdm`: For displaying progress bars during loops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "ac106b4c-cf2f-4ac3-9365-0aecf53eecaf",
    "_uuid": "4ecd403c-4153-4dc1-9840-c72f4a02a236",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-04-07T17:16:56.986552Z",
     "iopub.status.busy": "2025-04-07T17:16:56.986361Z",
     "iopub.status.idle": "2025-04-07T17:17:25.754834Z",
     "shell.execute_reply": "2025-04-07T17:17:25.753332Z",
     "shell.execute_reply.started": "2025-04-07T17:16:56.986533Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m482.8/482.8 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m21.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.0/85.0 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.2/59.2 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.4/10.4 MB\u001b[0m \u001b[31m75.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m354.7/354.7 kB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m481.2/481.2 kB\u001b[0m \u001b[31m22.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "!pip install datasets ftfy haystack-ai sentence-transformers bitsandbytes -q\n",
    "!pip install --upgrade transformers accelerate -q\n",
    "!pip install tqdm -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "72267add-2134-4b3e-a5bf-3ef8939747b0",
    "_uuid": "4fb88c29-4c83-457e-aec8-2410e90efb2b",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-04-07T17:17:25.756490Z",
     "iopub.status.busy": "2025-04-07T17:17:25.756224Z",
     "iopub.status.idle": "2025-04-07T17:17:57.371276Z",
     "shell.execute_reply": "2025-04-07T17:17:57.370533Z",
     "shell.execute_reply.started": "2025-04-07T17:17:25.756465Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# --- Core Library Imports ---\n",
    "import pandas as pd         # For data manipulation and saving results\n",
    "import torch              # PyTorch for tensor operations and GPU management\n",
    "import gc                 # Garbage collector for memory management\n",
    "import numpy as np          # Numerical operations (not heavily used here, but good practice)\n",
    "import time               # For timing experiments\n",
    "import json               # For saving experiment metadata (commented out currently)\n",
    "import os                 # For interacting with the operating system (paths, environment variables)\n",
    "import re                 # Regular expressions for text cleaning\n",
    "import nltk               # Natural Language Toolkit for sentence tokenization\n",
    "\n",
    "# --- Hugging Face Imports ---\n",
    "from datasets import load_dataset         # Function to load datasets from Hugging Face Hub\n",
    "from transformers import (\n",
    "    AutoTokenizer,                      # Loads tokenizers for LLMs\n",
    "    AutoModelForCausalLM,               # Loads causal LLMs (like Llama)\n",
    "    pipeline,                           # High-level interface for using models\n",
    "    BitsAndBytesConfig                  # Configuration for quantization\n",
    ")\n",
    "import huggingface_hub    # For interacting with the Hugging Face Hub (e.g., login)\n",
    "\n",
    "# --- Haystack Imports ---\n",
    "from haystack import Document             # Haystack's representation of text units\n",
    "from haystack.document_stores.in_memory import InMemoryDocumentStore # Simple document store\n",
    "from haystack.components.embedders import (\n",
    "    SentenceTransformersDocumentEmbedder, # Embeds Documents\n",
    "    SentenceTransformersTextEmbedder      # Embeds text queries\n",
    ")\n",
    "from haystack.components.retrievers.in_memory import InMemoryEmbeddingRetriever # Retrieves documents based on embedding similarity\n",
    "from haystack.utils.device import ComponentDevice # Utility for specifying device (CPU/GPU) for Haystack components\n",
    "\n",
    "# --- Utilities ---\n",
    "from tqdm.auto import tqdm # Progress bar utility\n",
    "import ftfy               # Text fixing library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "c43316fa-c8f6-47e7-a0bc-c74863831326",
    "_uuid": "39d6513a-d0a4-4435-93b9-b5a92f44f5c8",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Hugging Face Hub Authentication\n",
    "\n",
    "This token is required to download gated models like Llama 3.1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "1decf300-84ec-4e6b-8615-005e44b8ac16",
    "_uuid": "50c03653-c9f2-444b-9ba6-04722845dcd3",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-04-07T17:17:57.373399Z",
     "iopub.status.busy": "2025-04-07T17:17:57.372764Z",
     "iopub.status.idle": "2025-04-07T17:17:57.659666Z",
     "shell.execute_reply": "2025-04-07T17:17:57.659033Z",
     "shell.execute_reply.started": "2025-04-07T17:17:57.373376Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Set your Hugging Face token for accessing gated models\n",
    "huggingface_hub.login(\"Your_HuggingFace_Token_Here\") # Replace with your actual token or use a secure method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "0c61ac60-46c7-4555-ad2f-444496585c32",
    "_uuid": "f331444f-eb39-41ef-97a3-fb78af55c277",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Environment Setup and Data Loading\n",
    "This section configures the environment, downloads necessary NLTK data, checks GPU availability, and loads the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "ac7df0d3-2216-465e-9a8f-a8fb5881a62a",
    "_uuid": "382a9153-dfc1-48da-9cb3-37b25bb6e9f0",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-04-07T17:17:57.661240Z",
     "iopub.status.busy": "2025-04-07T17:17:57.660922Z",
     "iopub.status.idle": "2025-04-07T17:18:00.388532Z",
     "shell.execute_reply": "2025-04-07T17:18:00.387832Z",
     "shell.execute_reply.started": "2025-04-07T17:17:57.661210Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.5.1+cu121\n",
      "CUDA available: True\n",
      "Number of available GPUs: 2\n",
      "GPU 0: Tesla T4\n",
      "GPU 1: Tesla T4\n",
      "\n",
      "Loading datasets from Hugging Face Hub...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "527d62de1b95489488299154e02b66d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/719 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb40ba676b3345f99e6e79bcdc2cb9a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "part.0.parquet:   0%|          | 0.00/54.4k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25995964a6b54f7e9fd54f3cd5970752",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/918 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0bdeba6e6144ca795648ac2504f92b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "part.0.parquet:   0%|          | 0.00/797k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26b17147bd2e47daa7b7af8f9dfb13ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating passages split:   0%|          | 0/3200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded datasets.\n",
      "Loaded 918 test questions from QA dataset.\n"
     ]
    }
   ],
   "source": [
    "# --- Environment Configuration ---\n",
    "# Download the 'punkt' tokenizer models from NLTK, used for sentence splitting. 'quiet=True' suppresses output.\n",
    "nltk.download('punkt', quiet=True)\n",
    "# Setting for CUDA error reporting (can sometimes help debug GPU issues)\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "\n",
    "# --- Cache Directory Setup ---\n",
    "# Define and create a cache directory for Hugging Face models/datasets\n",
    "# This avoids re-downloading large files on subsequent runs.\n",
    "CACHE_DIR = \"./model_cache\"\n",
    "os.makedirs(CACHE_DIR, exist_ok=True)\n",
    "os.environ[\"HF_HOME\"] = CACHE_DIR # Set Hugging Face Hub's cache directory\n",
    "\n",
    "# --- GPU Availability Check ---\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Number of available GPUs: {torch.cuda.device_count()}\")\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "else:\n",
    "    print(\"CUDA not available. Running on CPU will be very slow.\")\n",
    "\n",
    "# --- Dataset Loading ---\n",
    "print(\"\\nLoading datasets from Hugging Face Hub...\")\n",
    "try:\n",
    "    # Load the question-answer pairs (test split)\n",
    "    QA_dataset = load_dataset(\"rag-datasets/rag-mini-wikipedia\", \"question-answer\", cache_dir=CACHE_DIR)\n",
    "    # Load the text corpus used for the knowledge base\n",
    "    text_dataset = load_dataset(\"rag-datasets/rag-mini-wikipedia\", \"text-corpus\", cache_dir=CACHE_DIR)\n",
    "    print(f\"Successfully loaded datasets.\")\n",
    "    # Print the number of test questions available\n",
    "    print(f\"Loaded {len(QA_dataset['test'])} test questions from QA dataset.\")\n",
    "    # Optional: Display dataset structure\n",
    "    # print(\"\\nQA Dataset Structure:\")\n",
    "    # print(QA_dataset)\n",
    "    # print(\"\\nText Corpus Dataset Structure:\")\n",
    "    # print(text_dataset)\n",
    "except Exception as e:\n",
    "    print(f\"Error loading datasets: {e}\")\n",
    "    print(\"Please ensure you are connected to the internet and the dataset name is correct.\")\n",
    "    # Depending on the error, you might want to exit or handle it differently\n",
    "    # exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "0ddbdb92-4686-40a0-b40e-5a3b0855aa58",
    "_uuid": "c228dcbc-475e-46bc-8cd1-9f6c646bd524",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Text Preprocessing and Chunking Strategy\n",
    "Defines functions to clean the text corpus and split it into manageable chunks for the RAG pipeline's knowledge base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "32d57891-4684-4a53-b000-b64f98552723",
    "_uuid": "8472d1ff-714e-43c1-b723-aa9ef02ef2d0",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-04-07T17:18:00.389719Z",
     "iopub.status.busy": "2025-04-07T17:18:00.389413Z",
     "iopub.status.idle": "2025-04-07T17:18:00.400070Z",
     "shell.execute_reply": "2025-04-07T17:18:00.399193Z",
     "shell.execute_reply.started": "2025-04-07T17:18:00.389686Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def preprocess_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Applies basic text cleaning operations.\n",
    "    - Fixes encoding issues using ftfy.\n",
    "    - Normalizes hyphens.\n",
    "    - Fixes escaped apostrophes.\n",
    "    - Removes content within parentheses (potential noise).\n",
    "\n",
    "    Args:\n",
    "        text (str): The input text string.\n",
    "\n",
    "    Returns:\n",
    "        str: The cleaned text string.\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str): # Basic type check\n",
    "        return \"\"\n",
    "    text = ftfy.fix_text(text) # Fix potential encoding errors (e.g., mojibake)\n",
    "    text = text.replace(\"--\", \"–\") # Replace double hyphens with en-dash\n",
    "    text = text.replace(\"\\\\'\", \"'\") # Replace escaped apostrophes\n",
    "    text = re.sub(r\"\\([^)]*\\)\", \"\", text) # Remove text within parentheses\n",
    "    return text.strip() # Remove leading/trailing whitespace\n",
    "\n",
    "\n",
    "def split_content_into_chunks(content: str, chunk_size: int = 100, overlap: int = 20) -> list[str]:\n",
    "    \"\"\"\n",
    "    Splits a large text content into smaller chunks suitable for embedding.\n",
    "\n",
    "    Strategy:\n",
    "    1. Splits the content by paragraphs (`\\n\\n`).\n",
    "    2. Iterates through paragraphs, adding them to the current chunk.\n",
    "    3. If adding a paragraph exceeds `chunk_size` (in words):\n",
    "       - Saves the current chunk.\n",
    "       - Starts a new chunk, potentially with `overlap` words from the end of the previous chunk.\n",
    "    4. If a single paragraph itself is larger than `chunk_size`:\n",
    "       - Splits that paragraph into sentences using `nltk.sent_tokenize`.\n",
    "       - Adds sentences one by one, creating new chunks with overlap as needed when `chunk_size` is exceeded.\n",
    "\n",
    "    Args:\n",
    "        content (str): The entire text corpus concatenated into a single string.\n",
    "        chunk_size (int): The target maximum number of words per chunk (approximate).\n",
    "        overlap (int): The target number of words to overlap between consecutive chunks.\n",
    "\n",
    "    Returns:\n",
    "        list[str]: A list of text chunks.\n",
    "    \"\"\"\n",
    "    if not isinstance(content, str) or not content:\n",
    "        return []\n",
    "\n",
    "    # First split by paragraphs, filtering out empty ones\n",
    "    paragraphs = [p for p in content.split('\\n\\n') if p.strip()]\n",
    "    if not paragraphs: # Handle case where content only has whitespace or is structured differently\n",
    "        # Fallback: split by single newline or sentence tokenize the whole thing\n",
    "        paragraphs = [p for p in content.split('\\n') if p.strip()]\n",
    "        if not paragraphs:\n",
    "             paragraphs = sent_tokenize(content) # Use sentence tokenization as last resort\n",
    "\n",
    "    chunks = []           # List to store the final chunks\n",
    "    current_chunk = []    # List of strings (paragraphs or sentences) forming the current chunk\n",
    "    current_size = 0      # Approximate word count of the current chunk\n",
    "\n",
    "    for paragraph in paragraphs:\n",
    "        paragraph_words = len(paragraph.split()) # Word count of the current paragraph\n",
    "\n",
    "        # --- Case 1: Paragraph itself is too large ---\n",
    "        if paragraph_words > chunk_size:\n",
    "            # Split large paragraph into sentences\n",
    "            sentences = sent_tokenize(paragraph)\n",
    "            for sentence in sentences:\n",
    "                sentence_words = len(sentence.split())\n",
    "\n",
    "                # If adding sentence exceeds chunk size, finalize previous chunk (if any)\n",
    "                if current_size + sentence_words > chunk_size and current_chunk:\n",
    "                    chunks.append(\" \".join(current_chunk)) # Save the completed chunk\n",
    "\n",
    "                    # Create overlap for the *next* chunk\n",
    "                    overlap_tokens = []\n",
    "                    overlap_size = 0\n",
    "                    # Iterate backwards through the *saved* chunk's pieces\n",
    "                    for piece in reversed(current_chunk):\n",
    "                         piece_words = len(piece.split())\n",
    "                         if overlap_size + piece_words <= overlap:\n",
    "                            overlap_tokens.append(piece)\n",
    "                            overlap_size += piece_words\n",
    "                         else:\n",
    "                             # If adding the whole piece exceeds overlap, try splitting it (optional, adds complexity)\n",
    "                             # For simplicity here, we just stop accumulating overlap tokens\n",
    "                             break # Stop if adding the next piece exceeds overlap target\n",
    "\n",
    "                    # Start new chunk with overlap (reversed back to original order)\n",
    "                    current_chunk = list(reversed(overlap_tokens))\n",
    "                    current_size = overlap_size\n",
    "\n",
    "                # Add the current sentence to the (potentially new) chunk\n",
    "                current_chunk.append(sentence)\n",
    "                current_size += sentence_words\n",
    "\n",
    "        # --- Case 2: Adding paragraph exceeds chunk size ---\n",
    "        elif current_size + paragraph_words > chunk_size and current_chunk:\n",
    "            # Finalize the current chunk before adding the new paragraph\n",
    "            chunks.append(\" \".join(current_chunk))\n",
    "\n",
    "            # Create overlap for the *next* chunk (similar logic as above)\n",
    "            overlap_tokens = []\n",
    "            overlap_size = 0\n",
    "            for piece in reversed(current_chunk):\n",
    "                piece_words = len(piece.split())\n",
    "                if overlap_size + piece_words <= overlap:\n",
    "                    overlap_tokens.append(piece)\n",
    "                    overlap_size += piece_words\n",
    "                else:\n",
    "                    break\n",
    "\n",
    "            # Start new chunk with overlap and the current paragraph\n",
    "            current_chunk = list(reversed(overlap_tokens))\n",
    "            current_size = overlap_size\n",
    "            current_chunk.append(paragraph)\n",
    "            current_size += paragraph_words\n",
    "\n",
    "        # --- Case 3: Paragraph fits into the current chunk ---\n",
    "        else:\n",
    "            current_chunk.append(paragraph)\n",
    "            current_size += paragraph_words\n",
    "\n",
    "    # Add the last remaining chunk if it's not empty\n",
    "    if current_chunk:\n",
    "        chunks.append(\" \".join(current_chunk))\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "1018253a-c3fe-4aca-bdc3-8b4743360372",
    "_uuid": "50f893ac-75ba-4ce4-8b0b-348ec6b53c7b",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Creating the RAG Knowledge Base with Haystack\n",
    "This section defines a function to prepare the Haystack `InMemoryDocumentStore`.\n",
    "It processes the text corpus, splits it into chunks based on the specified `chunk_size`,\n",
    "embeds these chunks using a sentence transformer model, and stores them in the document store.\n",
    "A cache (`document_stores_cache`) is used to avoid recomputing the store for the same `chunk_size`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_cell_guid": "a76fad61-d000-451b-a004-9c032e9b32c9",
    "_uuid": "071bad31-ef3b-4bc8-b722-9dbc558fd90b",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-04-07T17:18:00.401436Z",
     "iopub.status.busy": "2025-04-07T17:18:00.401138Z",
     "iopub.status.idle": "2025-04-07T17:18:00.428308Z",
     "shell.execute_reply": "2025-04-07T17:18:00.427502Z",
     "shell.execute_reply.started": "2025-04-07T17:18:00.401415Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Cache for document stores to avoid re-computation for the same chunk size\n",
    "document_stores_cache = {}\n",
    "\n",
    "def prepare_document_store(chunk_size: int) -> InMemoryDocumentStore:\n",
    "    \"\"\"\n",
    "    Prepares and populates an InMemoryDocumentStore with embedded text chunks.\n",
    "    Uses a cache (`document_stores_cache`) to return existing stores for a given chunk_size.\n",
    "\n",
    "    Args:\n",
    "        chunk_size (int): The target chunk size (in words) for splitting the corpus.\n",
    "\n",
    "    Returns:\n",
    "        InMemoryDocumentStore: A Haystack document store containing the embedded chunks.\n",
    "    \"\"\"\n",
    "    # Check cache first\n",
    "    if chunk_size in document_stores_cache:\n",
    "        print(f\"Using cached document store for chunk_size={chunk_size}\")\n",
    "        return document_stores_cache[chunk_size]\n",
    "\n",
    "    print(f\"\\n--- Preparing New Document Store (chunk_size={chunk_size}) ---\")\n",
    "    document_store = InMemoryDocumentStore(embedding_dim=384) # Dim matches all-MiniLM-L6-v2\n",
    "\n",
    "    # --- Step 1: Process and Concatenate Corpus ---\n",
    "    print(\"Processing and concatenating text corpus...\")\n",
    "    # Apply preprocessing to each passage and join them into one large string\n",
    "    # Using list comprehension with tqdm for progress bar\n",
    "    all_text_passages = [preprocess_text(entry['passage']) for entry in tqdm(text_dataset['passages'], desc=\"Preprocessing Passages\")]\n",
    "    all_text = \" \".join(filter(None, all_text_passages)) # Filter out potential empty strings after preprocessing\n",
    "    print(f\"Total characters in concatenated corpus: {len(all_text)}\")\n",
    "\n",
    "    # --- Step 2: Split into Chunks ---\n",
    "    print(\"Splitting content into chunks...\")\n",
    "    # Use the previously defined function with the specified chunk_size\n",
    "    # Using a default overlap of 20 words here, could be made configurable\n",
    "    content_chunks = split_content_into_chunks(all_text, chunk_size=chunk_size, overlap=20)\n",
    "    print(f\"Created {len(content_chunks)} chunks.\")\n",
    "    if not content_chunks:\n",
    "        print(\"Warning: No chunks were created. Check preprocessing and chunking logic.\")\n",
    "        # You might want to handle this case more robustly, maybe raise an error\n",
    "        # For now, return the empty store\n",
    "        document_stores_cache[chunk_size] = document_store\n",
    "        return document_store\n",
    "\n",
    "    # --- Step 3: Create Haystack Document Objects ---\n",
    "    # Convert each text chunk string into a Haystack Document object\n",
    "    docs = [Document(content=chunk) for chunk in content_chunks]\n",
    "    print(f\"Created {len(docs)} Haystack Document objects.\")\n",
    "\n",
    "    # --- Step 4: Embed Documents ---\n",
    "    print(\"Embedding documents...\")\n",
    "    # Determine the device for embedding (GPU if available, else CPU)\n",
    "    # Using Haystack's ComponentDevice for compatibility\n",
    "    if torch.cuda.is_available():\n",
    "        # Use the first GPU (cuda:0) for embedding by default\n",
    "        embedding_device = ComponentDevice.from_str(\"cuda:0\")\n",
    "        print(f\"Using device: {torch.cuda.get_device_name(0)} for document embedding.\")\n",
    "    else:\n",
    "        embedding_device = ComponentDevice.from_str(\"cpu\")\n",
    "        print(\"Using CPU for document embedding.\")\n",
    "\n",
    "    # Initialize the document embedder\n",
    "    doc_embedder = SentenceTransformersDocumentEmbedder(\n",
    "        model=\"sentence-transformers/all-MiniLM-L6-v2\", # A common, efficient embedding model\n",
    "        device=embedding_device, # Assign the determined device\n",
    "        batch_size=64,           # Adjust batch size based on GPU memory\n",
    "        progress_bar=True        # Show progress during embedding\n",
    "    )\n",
    "    # Warm up the embedder (loads the model)\n",
    "    try:\n",
    "        doc_embedder.warm_up()\n",
    "    except Exception as e:\n",
    "        print(f\"Error warming up document embedder: {e}\")\n",
    "        # Handle error appropriately, maybe fall back to CPU or exit\n",
    "        raise e # Re-raise the exception\n",
    "\n",
    "    # --- Step 5: Write Documents to Store (with embeddings) ---\n",
    "    # Process documents in batches to manage memory usage during embedding\n",
    "    write_batch_size = 128 # How many docs to embed and write at once\n",
    "    print(f\"Writing documents to store in batches of {write_batch_size}...\")\n",
    "    for i in tqdm(range(0, len(docs), write_batch_size), desc=\"Embedding and Writing Docs\"):\n",
    "        batch_docs = docs[i:i + write_batch_size]\n",
    "        try:\n",
    "            # Run embedding on the batch\n",
    "            docs_with_embeddings = doc_embedder.run(documents=batch_docs) # Ensure keyword arg 'documents' is used\n",
    "            # Write the embedded documents to the store\n",
    "            document_store.write_documents(docs_with_embeddings[\"documents\"])\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing or writing batch starting at index {i}: {e}\")\n",
    "            # Optional: Add logic to retry or skip the batch\n",
    "            continue # Continue with the next batch\n",
    "\n",
    "    # --- Caching and Return ---\n",
    "    print(f\"Document store preparation complete for chunk_size={chunk_size}.\")\n",
    "    document_stores_cache[chunk_size] = document_store # Cache the populated store\n",
    "    return document_store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "f597867e-4b5d-494c-99b4-08c40568672b",
    "_uuid": "0fe698c7-3b27-490c-a897-7895e297e2f5",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Debugging Functions (Optional)\n",
    "These functions were used during development to test specific components (retriever, RAG model, evaluator) in isolation with controlled inputs. They can be helpful for troubleshooting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_cell_guid": "230c12b5-d486-4a13-aa02-7987b509f6e6",
    "_uuid": "cd071c12-3b96-46ac-9b44-86b7f5f5cf79",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-04-07T17:18:00.429463Z",
     "iopub.status.busy": "2025-04-07T17:18:00.429148Z",
     "iopub.status.idle": "2025-04-07T17:18:00.452006Z",
     "shell.execute_reply": "2025-04-07T17:18:00.451235Z",
     "shell.execute_reply.started": "2025-04-07T17:18:00.429434Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# 1. Function to inspect retrieved documents for a given question\n",
    "def print_retrieved_documents(question: str, retrieved_docs: dict):\n",
    "    \"\"\"\n",
    "    Prints the question and the content of documents retrieved for it.\n",
    "\n",
    "    Args:\n",
    "        question (str): The input question.\n",
    "        retrieved_docs (dict): The output dictionary from a Haystack retriever run,\n",
    "                                expected to have a \"documents\" key.\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"DEBUG: Retrieved Documents for Question: {question}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    if \"documents\" in retrieved_docs and retrieved_docs[\"documents\"]:\n",
    "        for i, doc in enumerate(retrieved_docs[\"documents\"]):\n",
    "            print(f\"\\n--- DOCUMENT {i+1} (Score: {doc.score if hasattr(doc, 'score') else 'N/A'}) ---\")\n",
    "            print(f\"Content Length: {len(doc.content)} chars\")\n",
    "            print(f\"{'-'*60}\")\n",
    "            print(doc.content)\n",
    "            print(f\"{'-'*60}\")\n",
    "    else:\n",
    "        print(\"No documents retrieved or 'documents' key missing.\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "\n",
    "# 2. Function to test the core generation capability of the RAG model (Generator LLM)\n",
    "def test_rag_model(generator_model: 'ModelHandler', debug: bool = True):\n",
    "    \"\"\"\n",
    "    Performs a sanity check on the generator model with simple, predefined\n",
    "    context-question pairs. Bypasses the retrieval step.\n",
    "\n",
    "    Args:\n",
    "        generator_model (ModelHandler): An initialized instance of the ModelHandler class\n",
    "                                         for the generator LLM.\n",
    "        debug (bool): If True, prints detailed output. (Currently always prints).\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Testing RAG Generator Model (Direct Generation) ---\")\n",
    "    if not generator_model:\n",
    "        print(\"Error: Generator model not provided.\")\n",
    "        return\n",
    "\n",
    "    test_cases = [\n",
    "        # Add more cases if needed\n",
    "        {\n",
    "            \"question\": \"Was Abraham Lincoln the sixteenth President of the United States?\",\n",
    "            \"context\": \"Abraham Lincoln (February 12, 1809 – April 15, 1865) was an American lawyer and statesman who served as the 16th president of the United States from 1861 until his assassination in 1865.\",\n",
    "            \"expected_answer\": \"yes\" # Or a similar affirmation\n",
    "        },\n",
    "        {\n",
    "            \"question\": \"When did Lincoln begin his political career?\",\n",
    "            \"context\": \"Lincoln began his political career in 1832, at the age of 23, when he ran for the Illinois General Assembly.\",\n",
    "            \"expected_answer\": \"1832\"\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    for i, case in enumerate(test_cases):\n",
    "        question = case[\"question\"]\n",
    "        context = case[\"context\"]\n",
    "        expected = case[\"expected_answer\"]\n",
    "\n",
    "        # Simplified prompt, similar to the main pipeline but with controlled context\n",
    "        prompt = f\"\"\"Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer the question using only the context. Keep your answer brief.\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "        print(f\"\\n--- Test Case {i+1} ---\")\n",
    "        print(f\"Question: {question}\")\n",
    "        if debug: print(f\"Provided Context: '{context}'\")\n",
    "        print(f\"Expected Answer (approx): '{expected}'\")\n",
    "\n",
    "        # Generate with specific parameters for testing\n",
    "        try:\n",
    "            # Using a slightly larger max_tokens for testing to ensure output\n",
    "            raw_result = generator_model.generate(prompt, max_tokens=20, temperature=0.0) # Temp 0 for deterministic output\n",
    "            print(f\"Generated Answer: '{raw_result}'\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error during generation for test case {i+1}: {e}\")\n",
    "\n",
    "    print(\"\\n--- RAG Generator Model Testing Complete ---\")\n",
    "\n",
    "\n",
    "# 3. Function to test the Evaluator LLM\n",
    "def test_evaluator(evaluator_model: 'ModelHandler'):\n",
    "    \"\"\"\n",
    "    Performs a sanity check on the evaluator model with simple, predefined\n",
    "    question-answer pairs to see if it produces the expected \"Yes\" or \"No\".\n",
    "\n",
    "    Args:\n",
    "        evaluator_model (ModelHandler): An initialized instance of the ModelHandler class\n",
    "                                        for the evaluator LLM.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Testing Evaluator Model ---\")\n",
    "    if not evaluator_model:\n",
    "        print(\"Error: Evaluator model not provided.\")\n",
    "        return\n",
    "\n",
    "    test_cases = [\n",
    "        {\"question\": \"Is the sky blue?\", \"generated_answer\": \"Yes, the sky is blue.\", \"ground_truth\": \"yes\", \"expected_eval\": \"Yes\"},\n",
    "        {\"question\": \"Is the Earth flat?\", \"generated_answer\": \"No, the Earth is round.\", \"ground_truth\": \"no\", \"expected_eval\": \"Yes\"}, # Generated matches GT meaning\n",
    "        {\"question\": \"What is 2+2?\", \"generated_answer\": \"2\", \"ground_truth\": \"4\", \"expected_eval\": \"No\"},\n",
    "        {\"question\": \"What is 2+2?\", \"generated_answer\": \"4\", \"ground_truth\": \"4\", \"expected_eval\": \"Yes\"},\n",
    "        {\"question\": \"What is the capital of France?\", \"generated_answer\": \"Paris\", \"ground_truth\": \"Paris\", \"expected_eval\": \"Yes\"},\n",
    "        {\"question\": \"What is the capital of France?\", \"generated_answer\": \"Lyon\", \"ground_truth\": \"Paris\", \"expected_eval\": \"No\"},\n",
    "        {\"question\": \"Who was the first US president?\", \"generated_answer\": \"George Washington was the first president.\", \"ground_truth\": \"George Washington\", \"expected_eval\": \"Yes\"},\n",
    "        {\"question\": \"Who was the first US president?\", \"generated_answer\": \"\", \"ground_truth\": \"George Washington\", \"expected_eval\": \"No\"},\n",
    "    ]\n",
    "\n",
    "    for i, case in enumerate(test_cases):\n",
    "        query = case[\"question\"]\n",
    "        generated_answer = case[\"generated_answer\"]\n",
    "        ground_truth = case[\"ground_truth\"]\n",
    "        expected_eval = case[\"expected_eval\"]\n",
    "\n",
    "        # Evaluation prompt mirroring the main pipeline\n",
    "        prompt = f\"\"\"Task: In the context of the question, is the semantic meaning of the generated the same as the truth? Please reply \"Yes\" if true, else \"No.\"\n",
    "Question: {query}\n",
    "Generated: {generated_answer}\n",
    "Truth: {ground_truth}\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "        print(f\"\\n--- Test Case {i+1} ---\")\n",
    "        print(f\"Question: {query}\")\n",
    "        print(f\"Generated Answer: '{generated_answer}'\")\n",
    "        print(f\"Ground Truth: '{ground_truth}'\")\n",
    "        print(f\"Expected Evaluation: '{expected_eval}'\")\n",
    "\n",
    "        try:\n",
    "            # Generate evaluation using very few tokens and zero temperature\n",
    "            raw_result = evaluator_model.generate(prompt, max_tokens=3, temperature=0.0) # Increased slightly to catch variations\n",
    "            print(f\"Raw Evaluator Output: '{raw_result}'\")\n",
    "\n",
    "            # Check for expected output\n",
    "            if expected_eval.lower() in raw_result.lower():\n",
    "                print(\"Evaluation Result: Matches Expected\")\n",
    "            else:\n",
    "                print(\"Evaluation Result: *** Does Not Match Expected ***\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error during evaluation for test case {i+1}: {e}\")\n",
    "\n",
    "    print(\"\\n--- Evaluator Model Testing Complete ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "a5f7e14c-ff67-4767-b95e-58e87b82accd",
    "_uuid": "a1419416-5093-44f4-9a79-f986d1f21d18",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Model Handler Class\n",
    "A wrapper class to manage loading LLMs (with quantization) and generating text using the `transformers` pipeline. This simplifies using multiple models (generator, evaluator) potentially on different devices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_cell_guid": "3133d56d-cc06-4ed3-b9fc-cd0152e3448d",
    "_uuid": "68462209-d690-4b92-b467-92056f8a4fc8",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-04-07T17:18:00.453162Z",
     "iopub.status.busy": "2025-04-07T17:18:00.452854Z",
     "iopub.status.idle": "2025-04-07T17:18:00.476482Z",
     "shell.execute_reply": "2025-04-07T17:18:00.475669Z",
     "shell.execute_reply.started": "2025-04-07T17:18:00.453099Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "class ModelHandler:\n",
    "    \"\"\"\n",
    "    Manages loading and interacting with a Hugging Face causal language model,\n",
    "    specifically handling 8-bit quantization and text generation via pipelines.\n",
    "    \"\"\"\n",
    "    def __init__(self, model_name: str, device_id: int = 0):\n",
    "        \"\"\"\n",
    "        Initializes the ModelHandler.\n",
    "\n",
    "        Args:\n",
    "            model_name (str): The name of the model on Hugging Face Hub (e.g., \"meta-llama/Llama-3.1-8B\").\n",
    "            device_id (int): The GPU device ID to load the model onto (e.g., 0 for cuda:0).\n",
    "        \"\"\"\n",
    "        self.model_name = model_name\n",
    "        self.device_id = device_id\n",
    "        self.device = f\"cuda:{self.device_id}\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.tokenizer = None\n",
    "        self.model = None\n",
    "        self.pipe = None\n",
    "        print(f\"ModelHandler created for {self.model_name} on device target: {self.device}\")\n",
    "\n",
    "    def load_model(self):\n",
    "        \"\"\"\n",
    "        Loads the tokenizer and the specified LLM with 8-bit quantization.\n",
    "        Sets up the text generation pipeline.\n",
    "        \"\"\"\n",
    "        if self.pipe: # Avoid reloading if already loaded\n",
    "             print(f\"Model {self.model_name} already loaded.\")\n",
    "             return\n",
    "\n",
    "        print(f\"\\n--- Loading Model: {self.model_name} ---\")\n",
    "        try:\n",
    "            # --- Load Tokenizer ---\n",
    "            print(\"Loading tokenizer...\")\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(self.model_name, cache_dir=CACHE_DIR)\n",
    "            # Ensure a padding token is set; if not, use the EOS token\n",
    "            if self.tokenizer.pad_token is None:\n",
    "                print(\"Warning: Tokenizer does not have a pad token. Setting pad_token=eos_token.\")\n",
    "                self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "            print(\"Tokenizer loaded.\")\n",
    "\n",
    "            # --- GPU Memory Check (Optional but helpful) ---\n",
    "            if self.device != \"cpu\":\n",
    "                try:\n",
    "                    gpu_mem = torch.cuda.get_device_properties(self.device_id).total_memory\n",
    "                    gpu_mem_gb = gpu_mem / (1024**3)\n",
    "                    print(f\"Target GPU ({self.device}) total memory: {gpu_mem_gb:.2f} GB\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Could not get GPU memory info: {e}\")\n",
    "            else:\n",
    "                 print(\"Target device is CPU.\")\n",
    "\n",
    "\n",
    "            # --- Configure Quantization ---\n",
    "            print(\"Configuring 8-bit quantization...\")\n",
    "            quantization_config = BitsAndBytesConfig(\n",
    "                load_in_8bit=True,\n",
    "                # bnb_4bit_compute_dtype=torch.bfloat16 # This is for 4-bit, not needed for load_in_8bit=True\n",
    "            )\n",
    "            # Use bfloat16 for computations if available, otherwise float16 or float32\n",
    "            compute_dtype = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16\n",
    "\n",
    "\n",
    "            # --- Load Model ---\n",
    "            print(f\"Loading model {self.model_name} with quantization...\")\n",
    "            self.model = AutoModelForCausalLM.from_pretrained(\n",
    "                self.model_name,\n",
    "                torch_dtype=compute_dtype, # Use appropriate compute dtype\n",
    "                quantization_config=quantization_config,\n",
    "                low_cpu_mem_usage=True, # Tries to reduce CPU RAM usage during loading\n",
    "                device_map={\"\": self.device_id}, # Maps the entire model to the specified device ID\n",
    "                cache_dir=CACHE_DIR\n",
    "            )\n",
    "            print(\"Model loaded.\")\n",
    "\n",
    "            # --- Create Pipeline ---\n",
    "            # The pipeline handles tokenization, model inference, and decoding.\n",
    "            # `device_map` in from_pretrained already placed the model, so `device` arg in pipeline is not needed/can cause issues.\n",
    "            print(\"Creating text generation pipeline...\")\n",
    "            self.pipe = pipeline(\n",
    "                \"text-generation\",\n",
    "                model=self.model,\n",
    "                tokenizer=self.tokenizer,\n",
    "                # device=self.device # Not needed when device_map is used\n",
    "            )\n",
    "            print(f\"Pipeline created successfully for {self.model_name}.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"!!! ERROR loading model {self.model_name}: {e}\")\n",
    "            # Clean up partially loaded components\n",
    "            self.tokenizer = None\n",
    "            self.model = None\n",
    "            self.pipe = None\n",
    "            # Re-raise the exception to halt execution if loading fails\n",
    "            raise e\n",
    "\n",
    "    def generate(self, prompt: str, max_tokens: int = 100, temperature: float = 0.1) -> str:\n",
    "        \"\"\"\n",
    "        Generates text based on the input prompt using the loaded model pipeline.\n",
    "\n",
    "        Args:\n",
    "            prompt (str): The input text prompt for the model.\n",
    "            max_tokens (int): The maximum number of new tokens to generate.\n",
    "            temperature (float): Controls randomness. 0 for deterministic, >0 for more randomness.\n",
    "\n",
    "        Returns:\n",
    "            str: The generated text (only the response part, excluding the prompt).\n",
    "                 Returns \"Error in generation\" if an exception occurs.\n",
    "        \"\"\"\n",
    "        if self.pipe is None:\n",
    "            print(\"Model pipeline not loaded. Attempting to load...\")\n",
    "            try:\n",
    "                 self.load_model()\n",
    "            except Exception as e:\n",
    "                 print(f\"Failed to load model for generation: {e}\")\n",
    "                 return \"Error: Model not loaded\"\n",
    "\n",
    "\n",
    "        if not isinstance(prompt, str) or not prompt:\n",
    "             print(\"Warning: Empty or invalid prompt provided.\")\n",
    "             return \"\" # Return empty string for empty prompt\n",
    "\n",
    "\n",
    "        try:\n",
    "            # Generate response using the pipeline\n",
    "            # `max_new_tokens` controls the length of the generated part only\n",
    "            # `do_sample=True` is needed for temperature > 0\n",
    "            # Set pad_token_id to eos_token_id to suppress warnings when padding isn't explicitly handled elsewhere\n",
    "            response = self.pipe(\n",
    "                prompt,\n",
    "                max_new_tokens=max_tokens,\n",
    "                temperature=max(temperature, 1e-4), # Ensure temperature is slightly > 0 if sampling is desired\n",
    "                do_sample=(temperature > 0),\n",
    "                num_return_sequences=1,\n",
    "                pad_token_id=self.tokenizer.eos_token_id\n",
    "            )\n",
    "\n",
    "            # Extract only the generated text part\n",
    "            generated_full_text = response[0][\"generated_text\"]\n",
    "\n",
    "            # Check if the generated text contains the prompt and remove it\n",
    "            if generated_full_text.startswith(prompt):\n",
    "                 answer = generated_full_text[len(prompt):].strip()\n",
    "            else:\n",
    "                 # Sometimes the pipeline might not return the prompt; handle this gracefully\n",
    "                 print(\"Warning: Generated text did not start with the prompt. Returning full output.\")\n",
    "                 answer = generated_full_text.strip()\n",
    "\n",
    "\n",
    "            return answer\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"!!! ERROR during text generation: {e}\")\n",
    "            return \"Error in generation\" # Return a clear error message\n",
    "\n",
    "    def unload_model(self):\n",
    "        \"\"\"Explicitly delete model and tokenizer and clear GPU cache.\"\"\"\n",
    "        print(f\"Unloading model {self.model_name}...\")\n",
    "        del self.pipe\n",
    "        del self.model\n",
    "        del self.tokenizer\n",
    "        self.pipe = None\n",
    "        self.model = None\n",
    "        self.tokenizer = None\n",
    "        gc.collect() # Run Python garbage collection\n",
    "        if self.device != 'cpu':\n",
    "            torch.cuda.empty_cache() # Clear PyTorch's CUDA cache\n",
    "        print(f\"Model {self.model_name} unloaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b5efcab5-1c56-4e57-b499-d3fbc2df27a4",
    "_uuid": "845c36d0-2e36-46f9-bb09-34085c295a2e",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## RAG Pipeline: Answer Generation Function\n",
    "This function orchestrates the process of generating answers for a set of questions using the RAG approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_cell_guid": "8619308b-98e3-4338-bd9a-121ec456426c",
    "_uuid": "59a9cf4b-5d23-4095-a76f-d6f4150ecc32",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-04-07T17:18:00.478939Z",
     "iopub.status.busy": "2025-04-07T17:18:00.478728Z",
     "iopub.status.idle": "2025-04-07T17:18:00.503922Z",
     "shell.execute_reply": "2025-04-07T17:18:00.503325Z",
     "shell.execute_reply.started": "2025-04-07T17:18:00.478920Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def generate_answers(generator_model: ModelHandler,\n",
    "                     chunk_size: int,\n",
    "                     top_k: int,\n",
    "                     num_questions: int = None,\n",
    "                     debug: bool = False) -> list[dict]:\n",
    "    \"\"\"\n",
    "    Generates answers for questions using a RAG pipeline.\n",
    "\n",
    "    Steps:\n",
    "    1. Prepares/retrieves the document store for the given `chunk_size`.\n",
    "    2. Initializes the text embedder and retriever components from Haystack.\n",
    "    3. Iterates through the specified questions:\n",
    "       a. Embeds the question.\n",
    "       b. Retrieves the `top_k` relevant document chunks.\n",
    "       c. Formats a prompt including the retrieved context and the question.\n",
    "       d. Uses the `generator_model` to generate an answer based on the prompt.\n",
    "    4. Stores the question, generated answer, and ground truth answer.\n",
    "\n",
    "    Args:\n",
    "        generator_model (ModelHandler): Initialized handler for the generator LLM.\n",
    "        chunk_size (int): Chunk size used to prepare the document store.\n",
    "        top_k (int): The number of documents to retrieve for each question.\n",
    "        num_questions (int, optional): If provided, limits the number of questions processed.\n",
    "                                       Defaults to None (process all questions).\n",
    "        debug (bool): If True, prints detailed information during processing (retrieved docs, prompts).\n",
    "\n",
    "    Returns:\n",
    "        list[dict]: A list of dictionaries, each containing \"question\", \"generated_answer\",\n",
    "                    and \"ground_truth\" for one question.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Starting Answer Generation (chunk_size={chunk_size}, top_k={top_k}) ---\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    # --- Step 1: Prepare Document Store and Retriever ---\n",
    "    # Retrieve or create the document store using the helper function\n",
    "    document_store = prepare_document_store(chunk_size)\n",
    "    # Initialize the retriever component\n",
    "    retriever = InMemoryEmbeddingRetriever(document_store=document_store, top_k=top_k)\n",
    "\n",
    "    # --- Step 2: Prepare Query Embedder ---\n",
    "    # Determine device for query embedding (can be different from generator/evaluator)\n",
    "    # Place on GPU 1 if available and more than one GPU exists, otherwise GPU 0 or CPU\n",
    "    if torch.cuda.is_available():\n",
    "        embedder_device_id = 1 if torch.cuda.device_count() > 1 else 0\n",
    "        embedder_device_str = f\"cuda:{embedder_device_id}\"\n",
    "        print(f\"Using device {torch.cuda.get_device_name(embedder_device_id)} for query embedding.\")\n",
    "    else:\n",
    "        embedder_device_str = \"cpu\"\n",
    "        print(\"Using CPU for query embedding.\")\n",
    "    embedding_device = ComponentDevice.from_str(embedder_device_str)\n",
    "\n",
    "    # Initialize the text embedder for queries\n",
    "    query_embedder = SentenceTransformersTextEmbedder(\n",
    "        model=\"sentence-transformers/all-MiniLM-L6-v2\", # Must match document embedder\n",
    "        device=embedding_device,\n",
    "        progress_bar=False # Usually fast enough not to need a progress bar\n",
    "    )\n",
    "    # Warm up the embedder\n",
    "    try:\n",
    "        query_embedder.warm_up()\n",
    "    except Exception as e:\n",
    "        print(f\"Error warming up query embedder: {e}\")\n",
    "        raise e # Stop execution if embedder fails\n",
    "\n",
    "    # --- Step 3: Select Questions ---\n",
    "    questions_data = QA_dataset['test'] # Use the test split\n",
    "    if num_questions is not None and 0 < num_questions < len(questions_data):\n",
    "        print(f\"Processing a subset of {num_questions} questions.\")\n",
    "        questions_data = questions_data.select(range(num_questions))\n",
    "    else:\n",
    "        print(f\"Processing all {len(questions_data)} questions.\")\n",
    "\n",
    "    # --- Step 4: Prompt Template ---\n",
    "    # The placeholders {} will be filled with context and question.\n",
    "    prompt_template = \"\"\"Answer the question based *only* on the context provided below. Be concise and provide only the answer, without explanation or preamble.\n",
    "\n",
    "Context:\n",
    "{}\n",
    "\n",
    "Question: {}\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "    # --- Step 5: Iterate and Generate ---\n",
    "    results = [] # List to store results\n",
    "    # Loop through selected questions with a progress bar\n",
    "    for i in tqdm(range(len(questions_data)), desc=\"Generating Answers\"):\n",
    "        question = questions_data['question'][i]\n",
    "        ground_truth = questions_data['answer'][i]\n",
    "\n",
    "        if not isinstance(question, str) or not question.strip():\n",
    "             print(f\"Warning: Skipping invalid or empty question at index {i}\")\n",
    "             continue # Skip this iteration\n",
    "\n",
    "        try:\n",
    "            # Embed the current question\n",
    "            query_embedding_result = query_embedder.run(text=question)\n",
    "            query_embedding = query_embedding_result[\"embedding\"]\n",
    "\n",
    "            # Retrieve relevant documents using the embedding\n",
    "            retrieved_docs = retriever.run(query_embedding=query_embedding)\n",
    "\n",
    "            # --- Optional Debug Output ---\n",
    "            if debug:\n",
    "                 print_retrieved_documents(question, retrieved_docs) # Use the debug function\n",
    "\n",
    "            # Format the retrieved context\n",
    "            # Join the content of retrieved documents, separated by newlines\n",
    "            context_text = \"\\n\\n\".join([doc.content for doc in retrieved_docs[\"documents\"]])\n",
    "            if not context_text:\n",
    "                 print(f\"Warning: No context retrieved for question: {question}\")\n",
    "                 # Decide how to handle: skip, generate without context, or use a placeholder?\n",
    "                 # For now, generate with empty context.\n",
    "                 context_text = \"No context available.\"\n",
    "\n",
    "            # Construct the full prompt for the generator model\n",
    "            full_prompt = prompt_template.format(context_text, question)\n",
    "\n",
    "            if debug:\n",
    "                print(f\"\\n--- Prompt for Generator (Question {i+1}) ---\")\n",
    "                print(full_prompt)\n",
    "                print(\"-\" * 60)\n",
    "\n",
    "            # Generate the answer using the ModelHandler instance\n",
    "            # Using max_tokens=10 as specified in project summary, temperature=0 for consistency\n",
    "            generated_answer = generator_model.generate(full_prompt, max_tokens=10, temperature=0.0)\n",
    "\n",
    "            if debug:\n",
    "                print(f\"Raw Generated Answer: '{generated_answer}'\")\n",
    "                print(f\"Ground Truth: '{ground_truth}'\")\n",
    "                print(\"=\"*60)\n",
    "\n",
    "            # Basic validation/cleanup of the generated answer\n",
    "            if not generated_answer or generated_answer.isspace() or generated_answer == \"Error in generation\":\n",
    "                generated_answer = \"NO_RESPONSE\" # Use a placeholder for failed/empty generations\n",
    "\n",
    "            # Store the results for this question\n",
    "            results.append({\n",
    "                \"question\": question,\n",
    "                \"generated_answer\": generated_answer,\n",
    "                \"ground_truth\": ground_truth\n",
    "            })\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"!!! ERROR processing question {i+1} ('{question[:50]}...'): {e}\")\n",
    "            # Store error indicator for this question\n",
    "            results.append({\n",
    "                \"question\": question,\n",
    "                \"generated_answer\": \"GENERATION_ERROR\",\n",
    "                \"ground_truth\": ground_truth\n",
    "            })\n",
    "            continue # Continue to the next question\n",
    "\n",
    "    # --- Completion ---\n",
    "    elapsed_time = (time.time() - start_time) / 60\n",
    "    print(f\"--- Answer Generation Completed in {elapsed_time:.2f} minutes ---\")\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "5f47d2fd-75e4-4078-8c1f-cce1e82dd7ae",
    "_uuid": "aefcca5e-6a0f-4edc-9bdf-da97d6953fa5",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Evaluation Pipeline: Assessing Generated Answers\n",
    "This function takes the generated answers and uses the evaluator LLM to determine if the generated answer semantically matches the ground truth answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_cell_guid": "f3297525-fd9f-4d6d-9354-3824d74edc6e",
    "_uuid": "f886cd12-e5ea-4d97-b988-c92be0bad6c0",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-04-07T17:18:00.505059Z",
     "iopub.status.busy": "2025-04-07T17:18:00.504878Z",
     "iopub.status.idle": "2025-04-07T17:18:00.528039Z",
     "shell.execute_reply": "2025-04-07T17:18:00.527440Z",
     "shell.execute_reply.started": "2025-04-07T17:18:00.505043Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_answers(evaluator_model: ModelHandler,\n",
    "                     generated_results: list[dict],\n",
    "                     debug: bool = False) -> list[dict]:\n",
    "    \"\"\"\n",
    "    Evaluates the generated answers against ground truth using an evaluator LLM.\n",
    "\n",
    "    Steps:\n",
    "    1. Iterates through the results from the `generate_answers` function.\n",
    "    2. For each result:\n",
    "       a. Skips entries marked as \"NO_RESPONSE\" or \"GENERATION_ERROR\".\n",
    "       b. Formats a prompt asking the evaluator LLM to compare the generated answer\n",
    "          and ground truth, requesting a \"Yes\" or \"No\" response.\n",
    "       c. Uses the `evaluator_model` to get the evaluation result.\n",
    "    3. Stores the original info plus the evaluator's raw output (\"Evaluation\").\n",
    "\n",
    "    Args:\n",
    "        evaluator_model (ModelHandler): Initialized handler for the evaluator LLM.\n",
    "        generated_results (list[dict]): The list of dictionaries output by `generate_answers`.\n",
    "        debug (bool): If True, prints detailed information during the evaluation process.\n",
    "\n",
    "    Returns:\n",
    "        list[dict]: The input list augmented with an \"Evaluation\" key containing\n",
    "                    the raw output from the evaluator LLM.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Starting Answer Evaluation ---\")\n",
    "    start_time = time.time()\n",
    "    final_results_with_eval = [] # Store results including the evaluation\n",
    "\n",
    "    # --- Evaluation Prompt Template ---\n",
    "    # This prompt is crucial for instructing the evaluator model correctly.\n",
    "    # It clearly defines the task and the expected output format (\"Yes\" or \"No\").\n",
    "    eval_prompt_template = \"\"\"Task: In the context of the question, is the semantic meaning of the generated answer the same as the ground truth answer?\n",
    "Please reply *only* with the word \"Yes\" if they are semantically the same, or *only* with the word \"No\" if they are not. Do not provide any explanation.\n",
    "\n",
    "Question: {}\n",
    "Generated Answer: {}\n",
    "Ground Truth Answer: {}\n",
    "\n",
    "Evaluation:\"\"\" # Changed \"Answer:\" to \"Evaluation:\" for clarity\n",
    "\n",
    "    # --- Iterate and Evaluate ---\n",
    "    for entry in tqdm(generated_results, desc=\"Evaluating Answers\"):\n",
    "        query = entry[\"question\"]\n",
    "        generated_answer = entry[\"generated_answer\"]\n",
    "        ground_truth = entry[\"ground_truth\"]\n",
    "\n",
    "        # Handle cases where generation failed or produced no response\n",
    "        if generated_answer in [\"NO_RESPONSE\", \"GENERATION_ERROR\"]:\n",
    "            evaluation_output = \"N/A_GENERATION_FAILED\" # Mark evaluation as not applicable\n",
    "        else:\n",
    "            try:\n",
    "                # Construct the prompt for the evaluator\n",
    "                eval_prompt = eval_prompt_template.format(query, generated_answer, ground_truth)\n",
    "\n",
    "                if debug:\n",
    "                    print(f\"\\n--- Prompt for Evaluator ---\")\n",
    "                    print(eval_prompt)\n",
    "                    print(\"-\" * 60)\n",
    "\n",
    "                # Generate the evaluation using the evaluator model\n",
    "                # max_tokens=2 or 3 should be enough for \"Yes\" or \"No\" + potential whitespace/eos\n",
    "                # temperature=0 for deterministic evaluation\n",
    "                evaluation_output = evaluator_model.generate(eval_prompt, max_tokens=3, temperature=0.0)\n",
    "\n",
    "                # Basic cleanup of evaluator output (optional, depends on observed model behavior)\n",
    "                # evaluation_output = evaluation_output.strip().capitalize() # e.g., \"yes \" -> \"Yes\"\n",
    "\n",
    "                if debug:\n",
    "                    print(f\"Raw Evaluator Output: '{evaluation_output}'\")\n",
    "                    print(\"=\"*60)\n",
    "\n",
    "                # Handle potential errors during evaluation generation\n",
    "                if evaluation_output == \"Error in generation\":\n",
    "                    evaluation_output = \"EVALUATION_ERROR\"\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"!!! ERROR evaluating question ('{query[:50]}...'): {e}\")\n",
    "                evaluation_output = \"EVALUATION_ERROR\" # Mark as error\n",
    "\n",
    "        # Append original info and the evaluation result\n",
    "        final_results_with_eval.append({\n",
    "            \"question\": query,\n",
    "            \"generated_answer\": generated_answer,\n",
    "            \"ground_truth\": ground_truth,\n",
    "            \"Evaluation\": evaluation_output # Store the raw output from the evaluator\n",
    "        })\n",
    "\n",
    "    # --- Completion ---\n",
    "    elapsed_time = (time.time() - start_time) / 60\n",
    "    print(f\"--- Evaluation Completed in {elapsed_time:.2f} minutes ---\")\n",
    "    return final_results_with_eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "325baa42-9d0a-4320-9e18-02a2953e5375",
    "_uuid": "8369b350-73c6-424f-b4bd-6ee976ab04ab",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Experiment Execution Functions\n",
    "Functions to run a single experiment with specific parameters or multiple experiments iterating over parameter ranges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_cell_guid": "335ff12b-6378-46b6-982b-d7b7e1731b2b",
    "_uuid": "e4b2fed9-b991-4616-9245-d644125c1ade",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-04-07T17:18:00.529094Z",
     "iopub.status.busy": "2025-04-07T17:18:00.528910Z",
     "iopub.status.idle": "2025-04-07T17:18:00.555886Z",
     "shell.execute_reply": "2025-04-07T17:18:00.555022Z",
     "shell.execute_reply.started": "2025-04-07T17:18:00.529078Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def run_experiment(chunk_size: int,\n",
    "                   top_k: int,\n",
    "                   num_questions: int = None,\n",
    "                   debug: bool = False) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Runs a complete single experiment: loads models, generates answers, evaluates them,\n",
    "    and saves the results to a CSV file.\n",
    "\n",
    "    Args:\n",
    "        chunk_size (int): The chunk size for the document store.\n",
    "        top_k (int): The number of documents to retrieve.\n",
    "        num_questions (int, optional): Limit the number of questions for testing. Defaults to None (all).\n",
    "        debug (bool): Enables detailed print statements in sub-functions.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A pandas DataFrame containing the final results with evaluations.\n",
    "                      Returns an empty DataFrame if a critical error occurs.\n",
    "    \"\"\"\n",
    "    experiment_start_time = time.time()\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Starting Experiment: chunk_size={chunk_size}, top_k={top_k}\")\n",
    "    if num_questions:\n",
    "        print(f\"   (Using {num_questions} questions for this run)\")\n",
    "    print(f\"{'='*70}\")\n",
    "\n",
    "    generator_model = None\n",
    "    evaluator_model = None\n",
    "    results_df = pd.DataFrame() # Initialize empty DataFrame\n",
    "\n",
    "    try:\n",
    "        # --- Initialize Models ---\n",
    "        # Assign models to different GPUs if available (GPU 0 for Generator, GPU 1 for Evaluator)\n",
    "        gen_device_id = 0\n",
    "        eval_device_id = 1 if torch.cuda.device_count() > 1 else 0\n",
    "        print(f\"Assigning Generator to device cuda:{gen_device_id}\")\n",
    "        print(f\"Assigning Evaluator to device cuda:{eval_device_id}\")\n",
    "\n",
    "        # Instantiate model handlers\n",
    "        # Using Llama 3.1 8B for both generator and evaluator as per project summary\n",
    "        generator_model = ModelHandler(\"meta-llama/Llama-3.1-8B\", device_id=gen_device_id)\n",
    "        evaluator_model = ModelHandler(\"meta-llama/Llama-3.1-8B\", device_id=eval_device_id) # Changed from 1B\n",
    "\n",
    "        # --- Load Models ---\n",
    "        # Loading models sequentially can help manage memory demands\n",
    "        generator_model.load_model()\n",
    "        evaluator_model.load_model()\n",
    "\n",
    "        # --- Optional: Run Debug Tests ---\n",
    "        # These tests help verify models are loaded and behaving somewhat reasonably before the main run.\n",
    "        if debug:\n",
    "            test_rag_model(generator_model)\n",
    "            test_evaluator(evaluator_model)\n",
    "\n",
    "        # --- Step 1: Generate Answers ---\n",
    "        generated_results = generate_answers(\n",
    "            generator_model=generator_model,\n",
    "            chunk_size=chunk_size,\n",
    "            top_k=top_k,\n",
    "            num_questions=num_questions,\n",
    "            debug=debug # Pass debug flag down\n",
    "        )\n",
    "\n",
    "        # --- Step 2: Evaluate Answers ---\n",
    "        final_results_with_eval = evaluate_answers(\n",
    "            evaluator_model=evaluator_model,\n",
    "            generated_results=generated_results,\n",
    "            debug=debug # Pass debug flag down\n",
    "        )\n",
    "\n",
    "        # --- Step 3: Save Results ---\n",
    "        if final_results_with_eval:\n",
    "            results_df = pd.DataFrame(final_results_with_eval)\n",
    "            # Define filename including parameters and timestamp\n",
    "            timestamp = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "            base_filename = f'results_chunk{chunk_size}_top{top_k}_{timestamp}'\n",
    "            csv_filename = f'{base_filename}.csv'\n",
    "\n",
    "            print(f\"\\nSaving results to {csv_filename}...\")\n",
    "            results_df.to_csv(csv_filename, index=False)\n",
    "            print(f\"Results saved successfully ({len(results_df)} rows).\")\n",
    "\n",
    "            # --- Optional: Calculate and Print Accuracy based on Evaluator ---\n",
    "            # Note: This accuracy is based on the *evaluator LLM's* output (\"Yes\"/\"No\"),\n",
    "            # which is the subject of reliability analysis in the project.\n",
    "            # This calculation assumes the evaluator reliably outputs \"Yes\" or \"No\".\n",
    "            # You might need more robust parsing if the evaluator output varies.\n",
    "            # Count \"Yes\" (case-insensitive, stripping whitespace)\n",
    "            correct_count = results_df['Evaluation'].str.strip().str.lower().eq('yes').sum()\n",
    "            total_evaluated = len(results_df[results_df['Evaluation'] != \"N/A_GENERATION_FAILED\"]) # Exclude non-evaluated items\n",
    "            accuracy = (correct_count / total_evaluated) * 100 if total_evaluated > 0 else 0\n",
    "            print(f\"Evaluator-based Accuracy: {accuracy:.2f}% ({correct_count}/{total_evaluated})\")\n",
    "            # This accuracy is preliminary and needs comparison with human evaluation.\n",
    "\n",
    "        else:\n",
    "            print(\"Warning: No results were generated or evaluated.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"!!! CRITICAL ERROR during experiment (chunk={chunk_size}, top_k={top_k}): {e}\")\n",
    "        # Consider logging the full traceback\n",
    "        # import traceback\n",
    "        # traceback.print_exc()\n",
    "\n",
    "    finally:\n",
    "        # --- Step 4: Clean Up ---\n",
    "        # Ensure models are unloaded and memory is freed, even if errors occurred\n",
    "        print(\"\\nCleaning up models and freeing memory...\")\n",
    "        if generator_model:\n",
    "            generator_model.unload_model()\n",
    "        if evaluator_model:\n",
    "            evaluator_model.unload_model()\n",
    "        # Explicitly delete references and run GC again\n",
    "        del generator_model\n",
    "        del evaluator_model\n",
    "        gc.collect()\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "        print(\"Cleanup complete.\")\n",
    "\n",
    "        total_time_minutes = (time.time() - experiment_start_time) / 60\n",
    "        print(f\"\\n🏁 Experiment (chunk={chunk_size}, top_k={top_k}) Finished.\")\n",
    "        print(f\"   Total Time: {total_time_minutes:.2f} minutes.\")\n",
    "        print(f\"{'='*70}\\n\")\n",
    "\n",
    "    return results_df # Return the DataFrame (might be empty if errors occurred)\n",
    "\n",
    "\n",
    "# Function to run multiple experiments iterating through parameter lists.\n",
    "def run_multiple_experiments(chunk_sizes: list[int],\n",
    "                             top_k_values: list[int],\n",
    "                             num_questions: int = None):\n",
    "    \"\"\"\n",
    "    Runs a series of experiments for combinations of chunk sizes and top_k values.\n",
    "    Logs progress and saves a summary of (evaluator-based) results.\n",
    "\n",
    "    Args:\n",
    "        chunk_sizes (list[int]): List of chunk sizes to test.\n",
    "        top_k_values (list[int]): List of top_k values to test.\n",
    "        num_questions (int, optional): Limit the number of questions for each experiment run. Defaults to None (all).\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A summary DataFrame containing parameters and evaluator-based accuracy for each run.\n",
    "                      May be incomplete if errors occur.\n",
    "    \"\"\"\n",
    "    all_results_summary = [] # To store summary info from each run\n",
    "    overall_start_time = time.time()\n",
    "    log_filename = f\"experiment_log_{time.strftime('%Y%m%d-%H%M%S')}.txt\"\n",
    "\n",
    "    # --- Logging Setup ---\n",
    "    print(f\"Starting multiple experiments. Logging progress to: {log_filename}\")\n",
    "    with open(log_filename, \"w\") as log:\n",
    "        log.write(f\"--- Experiment Log ---\\n\")\n",
    "        log.write(f\"Start Time: {time.strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "        log.write(f\"Chunk Sizes: {chunk_sizes}\\n\")\n",
    "        log.write(f\"Top_k Values: {top_k_values}\\n\")\n",
    "        log.write(f\"Num Questions per run: {num_questions if num_questions else 'All'}\\n\")\n",
    "        log.write(\"-\" * 30 + \"\\n\\n\")\n",
    "\n",
    "    # --- Experiment Loop ---\n",
    "    total_experiments = len(chunk_sizes) * len(top_k_values)\n",
    "    current_experiment = 0\n",
    "    for chunk_size in chunk_sizes:\n",
    "        for top_k in top_k_values:\n",
    "            current_experiment += 1\n",
    "            print(f\"\\n>>> Running Experiment {current_experiment}/{total_experiments}: chunk={chunk_size}, top_k={top_k} <<<\")\n",
    "            with open(log_filename, \"a\") as log:\n",
    "                 log.write(f\"--- Starting Exp {current_experiment}/{total_experiments}: chunk={chunk_size}, top_k={top_k} ---\\n\")\n",
    "                 log.write(f\"Time: {time.strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "\n",
    "            try:\n",
    "                # Run the single experiment function\n",
    "                results_df = run_experiment(\n",
    "                    chunk_size=chunk_size,\n",
    "                    top_k=top_k,\n",
    "                    num_questions=num_questions,\n",
    "                    debug=False # Usually disable debug for multi-runs unless troubleshooting\n",
    "                )\n",
    "\n",
    "                # --- Log and Summarize Results ---\n",
    "                if not results_df.empty:\n",
    "                    # Recalculate evaluator accuracy for summary (robust parsing recommended)\n",
    "                    correct_count = results_df['Evaluation'].str.strip().str.lower().eq('yes').sum()\n",
    "                    total_evaluated = len(results_df[results_df['Evaluation'] != \"N/A_GENERATION_FAILED\"])\n",
    "                    accuracy = (correct_count / total_evaluated) * 100 if total_evaluated > 0 else 0\n",
    "                    run_time_minutes = (time.time() - overall_start_time) / 60 # Track time per experiment if needed\n",
    "\n",
    "                    summary_entry = {\n",
    "                        \"chunk_size\": chunk_size,\n",
    "                        \"top_k\": top_k,\n",
    "                        \"num_questions_processed\": len(results_df),\n",
    "                        \"evaluator_accuracy\": accuracy,\n",
    "                        # \"time_minutes\": run_time_minutes # Add if needed\n",
    "                    }\n",
    "                    all_results_summary.append(summary_entry)\n",
    "\n",
    "                    with open(log_filename, \"a\") as log:\n",
    "                        log.write(f\"Completed Successfully.\\n\")\n",
    "                        log.write(f\"Evaluator Accuracy: {accuracy:.2f}% ({correct_count}/{total_evaluated})\\n\")\n",
    "                        log.write(f\"Results saved to: results_chunk{chunk_size}_top{top_k}_*.csv\\n\")\n",
    "                        log.write(\"-\" * 20 + \"\\n\\n\")\n",
    "                else:\n",
    "                     with open(log_filename, \"a\") as log:\n",
    "                          log.write(f\"Completed with WARNINGS (Empty Results DataFrame).\\n\")\n",
    "                          log.write(\"-\" * 20 + \"\\n\\n\")\n",
    "\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"!!! FATAL ERROR in multi-experiment run (chunk={chunk_size}, top_k={top_k}): {e}\")\n",
    "                # Log the error\n",
    "                with open(log_filename, \"a\") as log:\n",
    "                    log.write(f\"!!! EXPERIMENT FAILED !!!\\n\")\n",
    "                    log.write(f\"Error: {str(e)}\\n\")\n",
    "                    # Consider logging traceback here too\n",
    "                    log.write(\"-\" * 20 + \"\\n\\n\")\n",
    "                # Optional: Decide whether to stop all runs or continue\n",
    "                # continue # Continue to the next experiment\n",
    "\n",
    "            # --- Intermediate Save (Optional but Recommended) ---\n",
    "            # Save the summary DataFrame periodically in case of crashes\n",
    "            if all_results_summary and current_experiment % 5 == 0: # Save every 5 experiments\n",
    "                temp_summary_df = pd.DataFrame(all_results_summary)\n",
    "                temp_summary_df.to_csv(f\"experiment_summary_progress_{time.strftime('%Y%m%d-%H%M%S')}.csv\", index=False)\n",
    "                print(f\"Saved intermediate progress summary ({current_experiment}/{total_experiments}).\")\n",
    "\n",
    "    # --- Final Summary ---\n",
    "    print(\"\\n--- All Experiments Attempted ---\")\n",
    "    overall_time_minutes = (time.time() - overall_start_time) / 60\n",
    "    print(f\"Total time for all experiments: {overall_time_minutes:.2f} minutes.\")\n",
    "\n",
    "    summary_df = pd.DataFrame(all_results_summary)\n",
    "    if not summary_df.empty:\n",
    "        print(\"\\nSummary of Results (based on Evaluator LLM):\")\n",
    "        print(summary_df)\n",
    "\n",
    "        # Save final summary\n",
    "        summary_filename_base = f\"experiment_summary_final_{time.strftime('%Y%m%d-%H%M%S')}\"\n",
    "        summary_df.to_csv(f\"{summary_filename_base}.csv\", index=False)\n",
    "        # summary_df.to_excel(f\"{summary_filename_base}.xlsx\", index=False) # Optional Excel export\n",
    "        print(f\"Final summary saved to {summary_filename_base}.csv\")\n",
    "\n",
    "        # --- Find Best Config (based on evaluator) ---\n",
    "        # Note: \"Best\" here is according to the possibly unreliable evaluator LLM\n",
    "        try:\n",
    "            best_idx = summary_df[\"evaluator_accuracy\"].idxmax()\n",
    "            best_config = summary_df.loc[best_idx]\n",
    "            print(\"\\nBest Configuration (according to Evaluator):\")\n",
    "            print(best_config)\n",
    "            with open(log_filename, \"a\") as log:\n",
    "                 log.write(f\"\\n--- Run Summary ---\\n\")\n",
    "                 log.write(f\"Total Time: {overall_time_minutes:.2f} minutes\\n\")\n",
    "                 log.write(f\"Best Config (Evaluator): chunk={best_config['chunk_size']}, k={best_config['top_k']}, Acc={best_config['evaluator_accuracy']:.2f}%\\n\")\n",
    "        except ValueError:\n",
    "             print(\"\\nCould not determine best configuration (summary might be empty or contain NaNs).\")\n",
    "             with open(log_filename, \"a\") as log:\n",
    "                  log.write(\"\\nCould not determine best configuration from summary.\\n\")\n",
    "\n",
    "    else:\n",
    "        print(\"\\nNo results were successfully summarized.\")\n",
    "        with open(log_filename, \"a\") as log:\n",
    "             log.write(\"\\n--- No results were successfully summarized. ---\\n\")\n",
    "\n",
    "\n",
    "    return summary_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "84c134f4-f4cb-4be6-bc34-6af728ba1740",
    "_uuid": "1b1fa1e4-e11d-46f5-9738-e7300b2927af",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Running the Experiments\n",
    "Define the parameter ranges and execute the experiment runs.\n",
    "Start with a small test run (`run_experiment`) before launching the full loop (`run_multiple_experiments`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_cell_guid": "525a1f71-3122-4373-94ea-f057bfb14bcc",
    "_uuid": "b8ba6756-830f-4224-968f-c7b9072ce8be",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-04-07T17:18:00.556977Z",
     "iopub.status.busy": "2025-04-07T17:18:00.556716Z",
     "iopub.status.idle": "2025-04-07T17:21:25.554936Z",
     "shell.execute_reply": "2025-04-07T17:21:25.554156Z",
     "shell.execute_reply.started": "2025-04-07T17:18:00.556951Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Running Single Test Experiment ---\n",
      "\n",
      "======================================================================\n",
      "Starting Experiment: chunk_size=150, top_k=3\n",
      "   (Using 10 questions for this run)\n",
      "======================================================================\n",
      "Assigning Generator to device cuda:0\n",
      "Assigning Evaluator to device cuda:1\n",
      "ModelHandler created for meta-llama/Llama-3.1-8B on device target: cuda:0\n",
      "ModelHandler created for meta-llama/Llama-3.1-8B on device target: cuda:1\n",
      "\n",
      "--- Loading Model: meta-llama/Llama-3.1-8B ---\n",
      "Loading tokenizer...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bcfe4e3250c7432a843821428df9f4a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/50.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a6887ed49ca4d8587443aa8b778e5be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee75ca755c794ae99468ed832904a565",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/73.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Tokenizer does not have a pad token. Setting pad_token=eos_token.\n",
      "Tokenizer loaded.\n",
      "Target GPU (cuda:0) total memory: 14.74 GB\n",
      "Configuring 8-bit quantization...\n",
      "Loading model meta-llama/Llama-3.1-8B with quantization...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7ce73fcdf2e4291accd8314064ce226",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/826 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87143352e35c401c97fd16f4c2823a50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "052828cd71314e30a61024073e7036cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a63fbcf9cccc489f857e239c2b5f0aa5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00004.safetensors:   0%|          | 0.00/1.17G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de25f693d10a4d69b9bce648490792b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7b39ce536164b799d2b8b00ae64f50f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b331011a40bb43a6b4cc9a1bc6740885",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "777883ad21934b9fb0de93e4fb790a94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bf622a0683440b7808bcf50a62666fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/185 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded.\n",
      "Creating text generation pipeline...\n",
      "Pipeline created successfully for meta-llama/Llama-3.1-8B.\n",
      "\n",
      "--- Loading Model: meta-llama/Llama-3.1-8B ---\n",
      "Loading tokenizer...\n",
      "Warning: Tokenizer does not have a pad token. Setting pad_token=eos_token.\n",
      "Tokenizer loaded.\n",
      "Target GPU (cuda:1) total memory: 14.74 GB\n",
      "Configuring 8-bit quantization...\n",
      "Loading model meta-llama/Llama-3.1-8B with quantization...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e20fe087c4d74efc8b18eae7496ceac6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded.\n",
      "Creating text generation pipeline...\n",
      "Pipeline created successfully for meta-llama/Llama-3.1-8B.\n",
      "\n",
      "--- Testing RAG Generator Model (Direct Generation) ---\n",
      "\n",
      "--- Test Case 1 ---\n",
      "Question: Was Abraham Lincoln the sixteenth President of the United States?\n",
      "Provided Context: 'Abraham Lincoln (February 12, 1809 – April 15, 1865) was an American lawyer and statesman who served as the 16th president of the United States from 1861 until his assassination in 1865.'\n",
      "Expected Answer (approx): 'yes'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0001` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Answer: 'Yes, Abraham Lincoln was the sixteenth President of the United States.'\n",
      "\n",
      "--- Test Case 2 ---\n",
      "Question: When did Lincoln begin his political career?\n",
      "Provided Context: 'Lincoln began his political career in 1832, at the age of 23, when he ran for the Illinois General Assembly.'\n",
      "Expected Answer (approx): '1832'\n",
      "Generated Answer: 'Lincoln began his political career in 1832, at the age of 23, when he ran'\n",
      "\n",
      "--- RAG Generator Model Testing Complete ---\n",
      "\n",
      "--- Testing Evaluator Model ---\n",
      "\n",
      "--- Test Case 1 ---\n",
      "Question: Is the sky blue?\n",
      "Generated Answer: 'Yes, the sky is blue.'\n",
      "Ground Truth: 'yes'\n",
      "Expected Evaluation: 'Yes'\n",
      "Raw Evaluator Output: 'Yes\n",
      "\n",
      "Explanation'\n",
      "Evaluation Result: Matches Expected\n",
      "\n",
      "--- Test Case 2 ---\n",
      "Question: Is the Earth flat?\n",
      "Generated Answer: 'No, the Earth is round.'\n",
      "Ground Truth: 'no'\n",
      "Expected Evaluation: 'Yes'\n",
      "Raw Evaluator Output: 'Yes\n",
      "\n",
      "Explanation'\n",
      "Evaluation Result: Matches Expected\n",
      "\n",
      "--- Test Case 3 ---\n",
      "Question: What is 2+2?\n",
      "Generated Answer: '2'\n",
      "Ground Truth: '4'\n",
      "Expected Evaluation: 'No'\n",
      "Raw Evaluator Output: 'No\n",
      "\n",
      "Explanation'\n",
      "Evaluation Result: Matches Expected\n",
      "\n",
      "--- Test Case 4 ---\n",
      "Question: What is 2+2?\n",
      "Generated Answer: '4'\n",
      "Ground Truth: '4'\n",
      "Expected Evaluation: 'Yes'\n",
      "Raw Evaluator Output: 'Yes\n",
      "\n",
      "Question'\n",
      "Evaluation Result: Matches Expected\n",
      "\n",
      "--- Test Case 5 ---\n",
      "Question: What is the capital of France?\n",
      "Generated Answer: 'Paris'\n",
      "Ground Truth: 'Paris'\n",
      "Expected Evaluation: 'Yes'\n",
      "Raw Evaluator Output: 'Yes\n",
      "\n",
      "Explanation'\n",
      "Evaluation Result: Matches Expected\n",
      "\n",
      "--- Test Case 6 ---\n",
      "Question: What is the capital of France?\n",
      "Generated Answer: 'Lyon'\n",
      "Ground Truth: 'Paris'\n",
      "Expected Evaluation: 'No'\n",
      "Raw Evaluator Output: 'No\n",
      "Explanation'\n",
      "Evaluation Result: Matches Expected\n",
      "\n",
      "--- Test Case 7 ---\n",
      "Question: Who was the first US president?\n",
      "Generated Answer: 'George Washington was the first president.'\n",
      "Ground Truth: 'George Washington'\n",
      "Expected Evaluation: 'Yes'\n",
      "Raw Evaluator Output: 'Yes\n",
      "\n",
      "Question'\n",
      "Evaluation Result: Matches Expected\n",
      "\n",
      "--- Test Case 8 ---\n",
      "Question: Who was the first US president?\n",
      "Generated Answer: ''\n",
      "Ground Truth: 'George Washington'\n",
      "Expected Evaluation: 'No'\n",
      "Raw Evaluator Output: 'Yes\n",
      "\n",
      "Question'\n",
      "Evaluation Result: *** Does Not Match Expected ***\n",
      "\n",
      "--- Evaluator Model Testing Complete ---\n",
      "\n",
      "--- Starting Answer Generation (chunk_size=150, top_k=3) ---\n",
      "\n",
      "--- Preparing New Document Store (chunk_size=150) ---\n",
      "!!! CRITICAL ERROR during experiment (chunk=150, top_k=3): InMemoryDocumentStore.__init__() got an unexpected keyword argument 'embedding_dim'\n",
      "\n",
      "Cleaning up models and freeing memory...\n",
      "Unloading model meta-llama/Llama-3.1-8B...\n",
      "Model meta-llama/Llama-3.1-8B unloaded.\n",
      "Unloading model meta-llama/Llama-3.1-8B...\n",
      "Model meta-llama/Llama-3.1-8B unloaded.\n",
      "Cleanup complete.\n",
      "\n",
      "🏁 Experiment (chunk=150, top_k=3) Finished.\n",
      "   Total Time: 3.42 minutes.\n",
      "======================================================================\n",
      "\n",
      "\n",
      "--- Test Run Complete ---\n"
     ]
    }
   ],
   "source": [
    "# --- Single Test Run (Recommended First) ---\n",
    "# Use a small number of questions and enable debug to verify the pipeline\n",
    "print(\"--- Running Single Test Experiment ---\")\n",
    "test_run_df = run_experiment(\n",
    "    chunk_size=150,      # Example parameters\n",
    "    top_k=3,\n",
    "    num_questions=10,   # Use a small number for testing\n",
    "    debug=True          # Enable debug output for inspection\n",
    ")\n",
    "print(\"\\n--- Test Run Complete ---\")\n",
    "# # Display the results of the test run\n",
    "# if not test_run_df.empty:\n",
    "#     print(test_run_df.head())\n",
    "# else:\n",
    "#     print(\"Test run produced no results (check logs/output for errors).\")\n",
    "\n",
    "\n",
    "# --- Full Experiment Runs ---\n",
    "# Uncomment and run this section to perform the full grid search.\n",
    "# **WARNING:** This will take a significant amount of time and compute resources.\n",
    "# Ensure you have sufficient GPU memory and time allocated. Monitor the process.\n",
    "\n",
    "# print(\"\\n--- !!! Starting Full Experiment Grid Search !!! ---\")\n",
    "# # Define the parameter grid based on the project description\n",
    "# chunk_sizes_to_test = [50, 100, 150, 200]\n",
    "# top_k_values_to_test = list(range(1, 11)) # [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "\n",
    "# # Run experiments for all combinations\n",
    "# # Set num_questions=None to use all 918 questions, or set a number for a smaller run.\n",
    "# full_summary_df = run_multiple_experiments(\n",
    "#     chunk_sizes=chunk_sizes_to_test,\n",
    "#     top_k_values=top_k_values_to_test,\n",
    "#     num_questions=None # Use None for all questions, or e.g., 50 for a smaller test\n",
    "# )\n",
    "\n",
    "# print(\"\\n--- !!! Full Experiment Grid Search Complete !!! ---\")\n",
    "# if not full_summary_df.empty:\n",
    "#      print(\"\\nFinal Summary DataFrame:\")\n",
    "#      print(full_summary_df)\n",
    "# else:\n",
    "#      print(\"Full experiment run produced no summary (check logs/output for errors).\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
